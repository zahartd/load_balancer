# HTTP-балансировщика нагрузки

Простой балансировщик нагрузки, который принимает входящие HTTP-запросы и распределяет их по пулу бэкенд-серверов + Rate limiter.

## Сборка и заупск

Все компоненты системы запускаются и собираются в docker контейнера (см. docker-compose.yaml и Dockerfile). Для удобства написан Makefile скрипт для быстрого запуска:

```bash
# Сборка и запуск балансировщика нагрузки + 3 тестовых бекендов
make start

# Остановка системы + удаление контейнеров, без удаления можно отправить сигнал через Ctrl+C или другим способом
make stop

# Запуск тестов на Golang юнит тесты и интеграционные (см. tests)
make tests

# Посмотреть логи контейнеров
make logs

# Нагрузочные тесты Apache Bench (В схеме с API-TOKEN это запросы от одного клиента)
make loadtest_ab

# Нагрузочные тесты Vegeta (10 разных клиентов: 80% 200 OK и 20 % 400, но это можно настроить в vegeta/targets.txt)
make loadtest_vegeta

# Локальная сборка бинаря
make build
```

Также можно запустить тесты локально:

```bash
# Go-тесты
go test -race ./...

# Нагрузочные тесты Apache Bench (см. доку AB)
ab -H "X-API-Key: client1" -n 5000 -c 1000 "http://localhost:8081/"

# Нагрузочные тесты Vegeta (см. доку https://github.com/tsenart/vegeta).
vegeta attack -targets="./vegeta/local.txt" -rate=1000 -duration=10s \
  | tee ./vegeta/reports/results.bin \
  | vegeta report --type=text
vegeta plot vegeta/reports/results.bin > vegeta/reports/plot.html
```

## Архитектруа решения

Структура файлов:

```bash
.
├── Dockerfile
├── Makefile
├── README.md
├── TASK.md
├── cmd
│   └── server
│       └── main.go
├── configs
│   └── config.json
├── docker-compose.yml
├── go.mod
├── go.sum
├── internal
│   ├── balancer
│   │   ├── algorithms
│   │   │   ├── roundrobin.go
│   │   │   └── roundrobin_test.go
│   │   ├── balancer.go
│   │   ├── balancer_factory.go
│   │   └── manager.go
│   ├── config
│   │   └── config.go
│   ├── gateways
│   │   └── http
│   │       ├── middleware.go
│   │       ├── proxy.go
│   │       └── server.go
│   ├── models
│   │   └── backend.go
│   └── ratelimit
│       ├── algorithms
│       │   ├── tokenbucket.go
│       │   └── tokenbucket_test.go
│       ├── limiter.go
│       ├── limiter_factory.go
│       └── manager.go
├── nginx
│   └── echo.conf
├── results.bin
├── tests
│   ├── balancer_test.go
│   ├── limiter_test.go
│   └── utils
│       └── switch_server.go
└── vegeta
    ├── local.txt
    ├── reports
    │   ├── plot.html
    │   └── results.bin
    └── targets.txt
```

В cmd/server лежит main.go - главный файл, входная точка приложения, в котором соединяем и запускаем все компоненты.
Все реализаци базируется в internal, прям разбить по слоям не удалось, но разбито по логичским частям:

**config** - тут все про конфигурацию приложения, все структуры конфигов и опций. 

**models** - какие ключевые модели приложения, пока тут только *Backend* - собственно инстанс бекенда к которому бы балансируем нагрузку. Также сюда в будущем отнесем объект клиента.

**gateways/http** - все что напрямую относится к публичному API и HTTP-серверу, тут лежит наша proxy-ручка перенаправляющая запросы на инстансы бекендов (и оркестрирующая через балансер их по разным бекендам), мидлваря с rate limiting-ом, непосредственно реализация http-сервера.

**balancer** - собственно сам балансер, реализован менеджер **LoadBalancer** который хранит в себе все список бекендов и алгоритм которым он по ним распределяет. Алгоритм определен в качестве интерфейса, рядом с местом применения, сами же реализации лежат в **algorithms**, пока реализован только Round Robin, но может еще какие-то появятся в будущем, в любом случае способ их добавления стандартизирован: добавляем новый алгоритм в отдельном файле на основе уже существующего алгоритма, определяем имя в конфигах и добавляем в фабрику балансеров новую ветку. Также при желание можно будет добавить опции для балансеров по аналогии с лимметерами.

P.S. Также манагер LoadBalancer отвечает и за поддержание актуального списка живых серверов, чтобы перенаправлять только на них, когда же сервер восстановиться его можно будет вернуть в "живые".

**ratelimit** - реализация Rate-Limiting. Структура почти такая же как у Load-Balancer, есть манагер, алгоритм лимитинга и конкретные реализации. Только в данном случае у нас свой экземпляр алгоритма на каждого клиента (взят уникальный API токен, передаваемый в хедере). Также есть фабрика алгоритмов, единый независимый интерфейс и реализации.

## Еще немного про инфраструктуру 

Все собирается и запускается в докер контейнерах. Для упрощенной реализации и нагрузочных тестов бекенды реализованы через инстанс nginx сервера с ручкой для пинга и общей руской которая всегда отдает 200 OK.

Контейнеры упорядочены чтобы запускаться в правильном порядке через условия.

Нагрузочные тесты также запускаются в отдельных контейнерах, но можно и локально поиграться.

## Про тестирирование

За % покрытия юнит-тестами не гнался пока что. Протестирваны с флагом -race самые "горячие" части: алгоритмы. А также написаны интеграционные тесты (tests):

**balancer_test.go** - запускаемый полноценные два сервера и собственно наш сервис балансера. Один сервер всегда жив, второй SwithServer - можно выставить живой он или нет. В этих тестах Load Balancer отключен (не передаем в конфиге). Тесты проверяют как сервис обрабатывает ситуации с временным отключением серверов и их восстановлением.

**limiter_test.go** - аналогично balancer_test, но тут проверяем уже именно логику Rate Limmiting: обработка по квотам (токенам) и восстановление квоты.

### Нагрузочные тесты ###

Использовал 2 интрумента: Apache Bench и Vegeta. У первого есть ограничение: неудобно делать много разных запросов (например, с разными заголовками и API Token в нашем случае). Поэтому этот тест показывает только случай когда нас дудосит один клиент, но видно на Rate Limiter не пропускает большую часть запросов, защищая сервера от перегрузки. Второй интструмент мне больше понравился - Vegeta, написан на Go, вроде много звезд и docker контейнер популярный. Позвволяет задать файл с запросами которые он будет по кругу гонять в заданном количестве, скоростью и времени отпраылять на наш сервис. Также можно построить график или передать в Prometheus/Grafana. В базовом варианте оставил тест с 8000 OK запросов и 2000 с 400 Bad Request, при этом "бомбит" нас 10 разных клиентов на протяжение 10 секунд. В целом за пару ms справляется со всеми запросами, если увеличивать давление, то могут начать появляться 429, где-то на 1 кк запросе вышли на пару секунд на все запросы, есть к чему стремиться. В так как у нас тут задачи однотипные, то упираемся в Rate Limiting, нужен либо более эффективный алгоритм (работающий одинаково хорошо для разных видов нагрузки), либо динамическая конфигурация настроек текущего. Ну pperf-ом пройтись еще не успел.

## Логирование

Особо не успел пока что, добавил просто много стандартных log.Printf и log.Fatalf, во многом используемые мной же для отладки. Но в будущем можно будет быстро переписать на более продвинутый инструмент.

## Что сделано из задания и что в планах

- [x] Основной функционал Балансировщика нагрузки
- [x] Распределение запросов по алгоритму round-robin
- [x] Обработана ситуация когда бекенды не доступны
- [x] Обеспечена корректная многоточная и асинхронная работа в условиях конкурентных вызовов
- [x] Ошибки обработаны и логируются
- [x] Самое базовое логгироввние через log
- [x] Конфигурация через config.json (список бекендов, настройки балансировщика и лимитера, а также хост и порт). Не зависит от кода, путь передается через переменные окружения.
- [x] Конфигурация через config.json (список бекендов, настройки балансировщика и лимитера, а также хост и порт). Не зависит от кода, путь передается через переменные окружения.
- [x] Базовая реализация Rate-Limiting. Реализвоан алгоритм Token Bucket.
- [x] Гранулярное ограничение, отслеживается состояние и лимиты каждого клиента, настройки пока что общие для всех клиентов.
- [x] Автоматическое и атомарное пополнение токенов.
- [x] Доступ к каждому бакету синхронизирован.
- [x] Добавлены базовые юнит, интеграционные и нагрузочные тесты.
- [x] Отслеживается здоровье бэкендов (Health Checks).
- [x] Graceful Shutdown
- [ ] Настройка каждого клиента и сохранения этого состояния в базу.
- [ ] CRUD для управления клиентами.
- [ ] Персистентность.
- [ ] Продвинутое логирование.
- [ ] Больше разных алгоритмов.


## Ответы на вопросы

> Опишите самую интересную задачу в программировании, которую вам приходилось решать?

Поиск (на самом деле приближение, так как это NP-сложная задача) самого длинного пути в полном взвешенном графе, прричем путь должен проходить через определенное число точек каждого из цветов. Длину пути считаем по метрики времени (сколько тратим на ребро + сколько проводим в каждом типе точки). Во-первых, это просто интересная алгоримичекая задача с разными подходама. Во-вторых, основной интерес этой задачи был в ее продуктовой цели (NDA) - улучшить опыт большого числа людей. Также кроме матичматической модели, у задачи было много технических ограничений, так нужно было продумать какие параметры искомого маршрута разрешить пользователю задать, какие данные можем достать из различных API, что пока (на время тестов) можно замокать. Особой проблемой была выгрузка этого огромного графа и его матрицы дистанций из внешнего API, тут пришлось придумать как делать это асинхронно, как можно разбить алгоритм на несколько независимых стадий и потом соединить результат, а также как динамически отдавать клиенту результаты и "перерисовывать" результат поиска. Это была крутая и интересная задача, писали на С++ (userver) с полноценным фронтендом на React. 

> Расскажите о своем самом большом факапе? Что вы предприняли для решения проблемы?

Прям яркого фокапа наверное вспомнить за последние годы не могу, так как в основном писал учебные, либо индивидуальные проекты. Но могу вспомнить случай почти 4 летней давности, когда я еще в 11 классе в составе команды пары моих однокласников залетели в один фриланс-стартап-проект (криптобиржа, лол). Я на том проекте был фронтедером и помню, что, во-первых, мой код тогда был не самый поддерживаемый, но так как читал его только я, это было не так важно (все-таки первый коммерческий проект), но значительным фокапом было не вставить на уровне клиента таймауты и блокировки на критические кнопки (api), такие как, напрмер, создание ордера (Take Profit или Stop loss), и проблема возникла у клиентов с плохой связью, из-за пролага запрос мог не сразу отправиться и клиент много раз зажимает кнопку (нервничает, этоже торговля), пролему нескольких ордеров на бекенде потом решили через идемпотентность, а вот нервы клиентов нужно было решить через визуальный эффект прогрогрузки и блокировки кнопки, чтобы клиенты не штурмовали тут же поддержку. В целом насколько помню на первых порах без отсутвия идемпотентности тогда наш заказчик потерял пару сотен тысяч, не приятно, но хорошо быстро заметили.

> Каковы ваши ожидания от участия в буткемпе?

Ожидаю поработать в команде с опытными инженерами, получить много интересных знаний и навыков, применить и улучшить уже имеющиеся, все-таки реальный прод имеет много в отличии от пет проектов, демо приложений и курсовых, много ньюансов, в том числе связанных с бизнесом и продуктовыми факторами, поэтому хочется поработать с реальным продуктом и реальными RPS так скажем, чтобы получить понимание как строяться большие проекты. В целом ожидаю, что после буткемпа будет более фундоментальное понимание как писать чистый и поддерживаемый код, так как как мне кажется только в команде можно этому навыку по настоящему научиться, а также продолжить развитие в области облачных вычислений и распрделенных систем.

